{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import hdf5_getters as getters\n",
    "import tables\n",
    "import tempfile\n",
    "import os\n",
    "import numpy as np\n",
    "import threading\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis(file, file_to_write):    \n",
    "    file_to_write.write(getters.get_song_id(file).decode('utf-8') + '|')\n",
    "    file_to_write.write(getters.get_song_hotttnesss(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_analysis_sample_rate(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_danceability(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_duration(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_end_of_fade_in(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_energy(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_key(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_key_confidence(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_loudness(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_mode(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_mode_confidence(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_start_of_fade_out(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_tempo(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_time_signature(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_time_signature_confidence(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_track_id(file).decode('utf-8') + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_segments_start(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_segments_confidence(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_segments_pitches(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_segments_timbre(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_segments_loudness_max(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_segments_loudness_max_time(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_segments_loudness_start(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_sections_start(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_sections_confidence(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_beats_start(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_beats_confidence(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_bars_start(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_bars_confidence(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_tatums_start(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "\n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_tatums_confidence(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp)\n",
    "    file_to_write.write('\\n')\n",
    "\n",
    "def generate_metadata(file, file_to_write):\n",
    "    file_to_write.write(getters.get_artist_familiarity(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_artist_hotttnesss(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_artist_id(file).decode('utf-8') + '|')\n",
    "    file_to_write.write(getters.get_artist_latitude(file).astype(str) + '|')\n",
    "    file_to_write.write(getters.get_artist_longitude(file).astype(str) + '|')\n",
    "    \n",
    "    temp = \"'\" + getters.get_artist_location(file).decode('utf-8', errors='replace').replace('\\ufffd', '?') + '\\''\n",
    "    file_to_write.write(temp + '|')\n",
    "    \n",
    "    temp = getters.get_artist_name(file).decode('utf-8', errors='replace').replace('\\ufffd', '?')\n",
    "    file_to_write.write(temp + '|')\n",
    "    \n",
    "    file_to_write.write(getters.get_song_id(file).decode('utf-8') + '|')\n",
    "    \n",
    "    temp = getters.get_title(file).decode('utf-8', errors='replace').replace('\\ufffd', '?')\n",
    "    file_to_write.write(temp + '|')\n",
    "    \n",
    "    temp = ' '.join([\"'\" + x.decode('utf-8') + \"'\" for x in getters.get_similar_artists(file)])\n",
    "    file_to_write.write(temp + '|')\n",
    "    \n",
    "    temp = ' '.join([\"'\" + x.decode('utf-8') + \"'\" for x in getters.get_artist_terms(file)])\n",
    "    file_to_write.write(temp + '|')\n",
    "    \n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_artist_terms_freq(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "    \n",
    "    temp = re.sub(r'[\\n\\t\\s]+', ' ', np.array2string(getters.get_artist_terms_weight(file).flatten(), precision=3, separator=' ', threshold=np.inf))\n",
    "    file_to_write.write(temp + '|')\n",
    "    \n",
    "    file_to_write.write(getters.get_year(file).astype(str))\n",
    "    file_to_write.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 100 files\n",
      "Finished processing 200 files\n",
      "Finished processing 300 files\n",
      "Finished processing 400 files\n",
      "Finished processing 500 files\n",
      "Finished processing 600 files\n",
      "Finished processing 700 files\n",
      "Finished processing 800 files\n",
      "Finished processing 900 files\n",
      "Finished processing 1000 files\n",
      "Finished processing 1100 files\n",
      "Finished processing 1200 files\n",
      "Finished processing 1300 files\n",
      "Finished processing 1400 files\n",
      "Finished processing 1500 files\n",
      "Finished processing 1600 files\n",
      "Finished processing 1700 files\n",
      "Finished processing 1800 files\n",
      "Finished processing 1900 files\n",
      "Finished processing 2000 files\n",
      "Finished processing 2100 files\n",
      "Finished processing 2200 files\n",
      "Finished processing 2300 files\n",
      "Finished processing 2400 files\n",
      "Finished processing 2500 files\n",
      "Finished processing 2600 files\n",
      "Finished processing 2700 files\n",
      "Finished processing 2800 files\n",
      "Finished processing 2900 files\n",
      "Finished processing 3000 files\n",
      "Finished processing 3100 files\n",
      "Finished processing 3200 files\n",
      "Finished processing 3300 files\n",
      "Finished processing 3400 files\n",
      "Finished processing 3500 files\n",
      "Finished processing 3600 files\n",
      "Finished processing 3700 files\n",
      "Finished processing 3800 files\n",
      "Finished processing 3900 files\n",
      "Finished processing 4000 files\n",
      "Finished processing 4100 files\n",
      "Finished processing 4200 files\n",
      "Finished processing 4300 files\n",
      "Finished processing 4400 files\n",
      "Finished processing 4500 files\n",
      "Finished processing 4600 files\n",
      "Finished processing 4700 files\n",
      "Finished processing 4800 files\n",
      "Finished processing 4900 files\n",
      "Finished processing 5000 files\n",
      "Finished processing 5100 files\n",
      "Finished processing 5200 files\n",
      "Finished processing 5300 files\n",
      "Finished processing 5400 files\n",
      "Finished processing 5500 files\n",
      "Finished processing 5600 files\n",
      "Finished processing 5700 files\n",
      "Finished processing 5800 files\n",
      "Finished processing 5900 files\n",
      "Finished processing 6000 files\n",
      "Finished processing 6100 files\n",
      "Finished processing 6200 files\n",
      "Finished processing 6300 files\n",
      "Finished processing 6400 files\n",
      "Finished processing 6500 files\n",
      "Finished processing 6600 files\n",
      "Finished processing 6700 files\n",
      "Finished processing 6800 files\n",
      "Finished processing 6900 files\n",
      "Finished processing 7000 files\n",
      "Finished processing 7100 files\n",
      "Finished processing 7200 files\n",
      "Finished processing 7300 files\n",
      "Finished processing 7400 files\n",
      "Finished processing 7500 files\n",
      "Finished processing 7600 files\n",
      "Finished processing 7700 files\n",
      "Finished processing 7800 files\n",
      "Finished processing 7900 files\n",
      "Finished processing 8000 files\n",
      "Finished processing 8100 files\n",
      "Finished processing 8200 files\n",
      "Finished processing 8300 files\n",
      "Finished processing 8400 files\n",
      "Finished processing 8500 files\n",
      "Finished processing 8600 files\n",
      "Finished processing 8700 files\n",
      "Finished processing 8800 files\n",
      "Finished processing 8900 files\n",
      "Finished processing 9000 files\n",
      "Finished processing 9100 files\n",
      "Finished processing 9200 files\n",
      "Finished processing 9300 files\n",
      "Finished processing 9400 files\n",
      "Finished processing 9500 files\n",
      "Finished processing 9600 files\n",
      "Finished processing 9700 files\n",
      "Finished processing 9800 files\n",
      "Finished processing 9900 files\n",
      "Finished processing 10000 files\n",
      "Finished processing all files. Exiting\n"
     ]
    }
   ],
   "source": [
    "analysis_file = open('data/analysis.txt', 'w', encoding='utf-8')\n",
    "metadata_file = open('data/metadata.txt', 'w', encoding='utf-8')\n",
    "file_count = 0\n",
    "\n",
    "with tarfile.open('millionsongsubset.tar.gz', 'r:gz') as tar:\n",
    "    for member in tar.getmembers():\n",
    "        if member.isfile():\n",
    "            file_count += 1\n",
    "            file_content = tar.extractfile(member).read()\n",
    "            \n",
    "            temp_file = tempfile.NamedTemporaryFile(suffix='.h5', delete=False)\n",
    "            temp_file.write(file_content)\n",
    "            temp_file_name = temp_file.name\n",
    "            \n",
    "            h5file = tables.open_file(temp_file_name)\n",
    "            try:\n",
    "                generate_analysis(h5file, analysis_file)\n",
    "                generate_metadata(h5file, metadata_file)\n",
    "            except: print(f'Exception with fie {member.name}')\n",
    "            finally:\n",
    "                h5file.close()\n",
    "            \n",
    "            temp_file.close()\n",
    "            os.remove(temp_file_name)\n",
    "            \n",
    "            if file_count % 100 == 0: print(f'Finished processing {file_count} files')\n",
    "\n",
    "analysis_file.close()\n",
    "metadata_file.close()\n",
    "print('Finished processing all files. Exiting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis = pd.read_csv('data/analysis.txt', header=None, delimiter='|')\n",
    "df_analysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata = pd.read_csv('data/metadata.txt', header=None, delimiter='|')\n",
    "df_metadata.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
